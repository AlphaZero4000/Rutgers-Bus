{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d636c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f86718c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              name\n",
      "0          Systems\n",
      "1           Routes\n",
      "2            Buses\n",
      "3            Stops\n",
      "4      Route_Stops\n",
      "5         ETA_Logs\n",
      "6         Bus_Logs\n",
      "7  sqlite_sequence\n",
      "(1696450, 8)\n"
     ]
    }
   ],
   "source": [
    "db_file = \"rutgers_buses.db\"\n",
    "\n",
    "with sqlite3.connect(db_file) as con:\n",
    "    tables = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", con)\n",
    "    print(tables)\n",
    "    \n",
    "    df = pd.read_sql_query(\"SELECT * FROM Bus_Logs\", con)\n",
    "    names = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", con)['name'].tolist()\n",
    "    dfs = {name: pd.read_sql_query(f\"SELECT * FROM \\\"{name}\\\";\", con) for name in names}\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2b49d7",
   "metadata": {},
   "source": [
    "Haversine distance formula is used to find speed later on. I found this formula online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f4fe2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000.0\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "    deltaphi = np.radians(lat2 - lat1)\n",
    "    deltalambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(deltaphi/2.0)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(deltalambda/2.0)**2\n",
    "    return R * 2 * np.arcsin(np.sqrt(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90177773",
   "metadata": {},
   "source": [
    "This cell will do feature engineering and calculate the actual arrival time as well as the instantaneous speed vs the 1 min average speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea353d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/zg72qj2521b4v_sw10nk4ntm0000gn/T/ipykernel_14877/1806519234.py:59: RuntimeWarning: divide by zero encountered in divide\n",
      "  speed = dist / dt\n",
      "/var/folders/j7/zg72qj2521b4v_sw10nk4ntm0000gn/T/ipykernel_14877/1806519234.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  speed = dist / dt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "route: 26280 sample next mappings: [('10035', '27767'), ('27767', '10038'), ('10038', '10035'), ('10034', '10041'), ('10041', '10052'), ('10052', '10071'), ('10071', '10029'), ('10029', '10065'), ('10065', '12913'), ('12913', '10037'), ('10037', '10059'), ('10059', '10042'), ('10042', '10061'), ('10061', '10036'), ('10036', '62662'), ('62662', '10038')]\n",
      "route: 26281 sample next mappings: [('10035', '27767'), ('27767', '10075'), ('10075', '10037'), ('10037', '10059'), ('10059', '10042'), ('10042', '10026'), ('10026', '10061'), ('10061', '10036'), ('10036', '10038'), ('10038', '10071'), ('10071', '10029'), ('10029', '10065'), ('10065', '10052'), ('10052', '10039'), ('10039', '21050'), ('21050', '27767')]\n",
      "route: 26435 sample next mappings: [('10035', '27767'), ('27767', '10038'), ('10038', '10035'), ('10034', '10041'), ('10041', '10052'), ('10052', '10071'), ('10071', '10029'), ('10029', '12913'), ('12913', '10037'), ('10037', '10059'), ('10059', '10042'), ('10042', '10026'), ('10026', '10061'), ('10061', '10036'), ('10036', '62662'), ('62662', '10038')]\n",
      "route: 26436 sample next mappings: [('10035', '27767'), ('27767', '10075'), ('10075', '10037'), ('10037', '10059'), ('10059', '10042'), ('10042', '10026'), ('10026', '10061'), ('10061', '10036'), ('10036', '10038'), ('10038', '10071'), ('10071', '10029'), ('10029', '10052'), ('10052', '10039'), ('10039', '21050'), ('21050', '10035')]\n",
      "route: 37199 sample next mappings: [('10035', '27767'), ('27767', '10038'), ('10038', '27767'), ('10034', '10041'), ('10041', '10052'), ('10052', '10071'), ('10071', '10029'), ('10029', '10065'), ('10065', '12913'), ('12913', '10037'), ('10037', '10059'), ('10059', '10042'), ('10042', '10026'), ('10026', '10061'), ('10061', '10036'), ('10036', '62662'), ('62662', '10038')]\n",
      "route: 4056 sample next mappings: [('10098', '10053'), ('10053', '10066'), ('10066', '10057'), ('10057', '10079'), ('10079', '10054'), ('10054', '10033'), ('10033', '10090'), ('10090', '10027'), ('10027', '10098')]\n",
      "route: 4063 sample next mappings: [('10056', '10090'), ('10090', '10033'), ('10033', '10055'), ('10055', '10053'), ('10053', '10093'), ('10093', '10058'), ('10058', '10074'), ('10074', '10076'), ('10076', '10050'), ('10050', '10047'), ('10047', '10056')]\n",
      "route: 4088 sample next mappings: [('10056', '10053'), ('10053', '10058'), ('10058', '10074'), ('10074', '10056')]\n",
      "route: 4098 sample next mappings: [('10098', '10053'), ('10053', '10066'), ('10066', '10098')]\n",
      "route: 41231 sample next mappings: [('22103', '22104'), ('22104', '22105'), ('22105', '22106'), ('22106', '23404'), ('23404', '22107'), ('22107', '22103')]\n",
      "route: 43990 sample next mappings: [('153546', '123068'), ('123068', '123068')]\n",
      "route: 43991 sample next mappings: [('153546', '153552'), ('153552', '153552')]\n",
      "route: 54540 sample next mappings: [('10035', '27767'), ('27767', '10038'), ('10038', '10060'), ('10060', '10034'), ('10034', '10041'), ('10041', '10052'), ('10052', '10089'), ('10089', '10035')]\n",
      "route: 54541 sample next mappings: [('10029', '10065'), ('10065', '10034'), ('10034', '10041'), ('10041', '10052'), ('10052', '10071'), ('10071', '10029')]\n",
      "route: 54543 sample next mappings: [('10035', '27767'), ('27767', '10075'), ('10075', '10037'), ('10037', '10059'), ('10059', '10042'), ('10042', '10026'), ('10026', '10061'), ('10061', '10036'), ('10036', '62662'), ('62662', '10038'), ('10038', '10035')]\n",
      "route: 54544 sample next mappings: [('10037', '10059'), ('10059', '10036'), ('10036', '10038'), ('10038', '27767'), ('27767', '10037')]\n",
      "route: 54545 sample next mappings: [('10029', '10065'), ('10065', '10035'), ('10035', '27767'), ('27767', '10038'), ('10038', '10071'), ('10071', '10029')]\n",
      "route: 54550 sample next mappings: [('10037', '10059'), ('10059', '10036'), ('10036', '10034'), ('10034', '10039'), ('10039', '21050'), ('21050', '10037')]\n",
      "route: 54551 sample next mappings: [('10037', '10059'), ('10059', '10036'), ('10036', '10071'), ('10071', '10029'), ('10029', '10037')]\n",
      "route: 55366 sample next mappings: [('10060', '10034'), ('10034', '10039'), ('10039', '21050'), ('21050', '10060')]\n",
      "route: 55368 sample next mappings: [('10052', '10039'), ('10039', '21050'), ('21050', '10060'), ('10060', '10035'), ('10035', '27767'), ('27767', '10038'), ('10038', '10089'), ('10089', '10052')]\n",
      "route: 59451 sample next mappings: [('153546', '10029'), ('10029', '10065'), ('10065', '10052'), ('10052', '188704'), ('188704', '10060'), ('10060', '10034'), ('10034', '10041'), ('10041', '153546')]\n",
      "Chunk 1/56 — rows 1 / 11153406\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 207\u001b[39m\n\u001b[32m    205\u001b[39m         index += \u001b[32m1\u001b[39m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.NaT\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m merged_valid[\u001b[33m\"\u001b[39m\u001b[33mactual_arrival_ts\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mmerged_valid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_arrival_ts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbus_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstart_ts\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroute_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mroute_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    210\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m merged_valid[\u001b[33m\"\u001b[39m\u001b[33mpred_eta_s\u001b[39m\u001b[33m\"\u001b[39m] = pd.to_numeric(merged_valid[\u001b[33m\"\u001b[39m\u001b[33meta_seconds\u001b[39m\u001b[33m\"\u001b[39m], errors=\u001b[33m\"\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    212\u001b[39m merged_valid[\u001b[33m\"\u001b[39m\u001b[33mactual_travel_s\u001b[39m\u001b[33m\"\u001b[39m] = (pd.to_datetime(merged_valid[\u001b[33m\"\u001b[39m\u001b[33mactual_arrival_ts\u001b[39m\u001b[33m\"\u001b[39m]) - pd.to_datetime(merged_valid[\u001b[33m\"\u001b[39m\u001b[33mstart_ts\u001b[39m\u001b[33m\"\u001b[39m])).dt.total_seconds()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/frame.py:10374\u001b[39m, in \u001b[36mDataFrame.apply\u001b[39m\u001b[34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m  10360\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[32m  10362\u001b[39m op = frame_apply(\n\u001b[32m  10363\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  10364\u001b[39m     func=func,\n\u001b[32m   (...)\u001b[39m\u001b[32m  10372\u001b[39m     kwargs=kwargs,\n\u001b[32m  10373\u001b[39m )\n\u001b[32m> \u001b[39m\u001b[32m10374\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/apply.py:916\u001b[39m, in \u001b[36mFrameApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_raw(engine=\u001b[38;5;28mself\u001b[39m.engine, engine_kwargs=\u001b[38;5;28mself\u001b[39m.engine_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/apply.py:1063\u001b[39m, in \u001b[36mFrameApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1062\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine == \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m         results, res_index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1064\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m         results, res_index = \u001b[38;5;28mself\u001b[39m.apply_series_numba()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/apply.py:1081\u001b[39m, in \u001b[36mFrameApply.apply_series_generator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1078\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1079\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[32m   1080\u001b[39m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1081\u001b[39m         results[i] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1082\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[32m   1083\u001b[39m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[32m   1084\u001b[39m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[32m   1085\u001b[39m             results[i] = results[i].copy(deep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 208\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(r)\u001b[39m\n\u001b[32m    205\u001b[39m         index += \u001b[32m1\u001b[39m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.NaT\n\u001b[32m    207\u001b[39m merged_valid[\u001b[33m\"\u001b[39m\u001b[33mactual_arrival_ts\u001b[39m\u001b[33m\"\u001b[39m] = merged_valid.apply(\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m r: \u001b[43mlookup_arrival_ts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbus_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstart_ts\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroute_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mroute_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    209\u001b[39m     axis=\u001b[32m1\u001b[39m\n\u001b[32m    210\u001b[39m )\n\u001b[32m    211\u001b[39m merged_valid[\u001b[33m\"\u001b[39m\u001b[33mpred_eta_s\u001b[39m\u001b[33m\"\u001b[39m] = pd.to_numeric(merged_valid[\u001b[33m\"\u001b[39m\u001b[33meta_seconds\u001b[39m\u001b[33m\"\u001b[39m], errors=\u001b[33m\"\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    212\u001b[39m merged_valid[\u001b[33m\"\u001b[39m\u001b[33mactual_travel_s\u001b[39m\u001b[33m\"\u001b[39m] = (pd.to_datetime(merged_valid[\u001b[33m\"\u001b[39m\u001b[33mactual_arrival_ts\u001b[39m\u001b[33m\"\u001b[39m]) - pd.to_datetime(merged_valid[\u001b[33m\"\u001b[39m\u001b[33mstart_ts\u001b[39m\u001b[33m\"\u001b[39m])).dt.total_seconds()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 196\u001b[39m, in \u001b[36mlookup_arrival_ts\u001b[39m\u001b[34m(bid, stop_id, start_ts, route_id)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m index < arr[\u001b[33m\"\u001b[39m\u001b[33mts\u001b[39m\u001b[33m\"\u001b[39m].size:\n\u001b[32m    195\u001b[39m     event_stop = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(arr[\u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m][index]))\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     delta_s = (arr[\u001b[33m\"\u001b[39m\u001b[33mts\u001b[39m\u001b[33m\"\u001b[39m][index] - st64) / \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtimedelta64\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ms\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m next_stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m event_stop == \u001b[38;5;28mstr\u001b[39m(next_stop):\n\u001b[32m    198\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m pd.NaT\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "DB_FILE = \"rutgers_buses.db\"\n",
    "BUS_TBL = \"Bus_Logs\"\n",
    "ETA_TBL = \"ETA_Logs\"\n",
    "\n",
    "BUS_LOG_ID = \"log_id\"\n",
    "BUS_TS = \"timestamp\"\n",
    "BUS_BUS_ID = \"bus_id\"\n",
    "BUS_LAT = \"latitude\"\n",
    "BUS_LON = \"longitude\"\n",
    "BUS_PAX = \"pax_load\"\n",
    "BUS_ARRIVED_STOP = \"arrived_stop_id\"\n",
    "\n",
    "ETA_LOG_ID = \"log_id\"\n",
    "ETA_STOP = \"stop_id\"\n",
    "ETA_SECONDS = \"eta_seconds\"\n",
    "ETA_SORT = \"sort_order\"\n",
    "\n",
    "MAX_TIME_BEFORE_STOP = 60 * 60 \n",
    "CHUNKSIZE = 200000\n",
    "\n",
    "with sqlite3.connect(DB_FILE) as con:\n",
    "    bus = pd.read_sql_query(f\"SELECT * FROM {BUS_TBL}\", con)\n",
    "    eta_cols = pd.read_sql_query(f\"PRAGMA table_info('{ETA_TBL}')\", con)['name'].tolist()\n",
    "\n",
    "required_bus_cols = {BUS_LOG_ID, BUS_TS, BUS_BUS_ID, BUS_LAT, BUS_LON, BUS_PAX, BUS_ARRIVED_STOP}\n",
    "missing = required_bus_cols - set(bus.columns)\n",
    "\n",
    "\n",
    "route_col_in_bus = next((c for c in bus.columns if \"route\" in c.lower() and \"id\" in c.lower()), None)\n",
    "\n",
    "bus = bus.copy()\n",
    "timestamp_values = pd.to_numeric(bus[BUS_TS], errors=\"coerce\").dropna()\n",
    "unit = \"s\"\n",
    "\n",
    "bus[BUS_LAT] = pd.to_numeric(bus[BUS_LAT], errors=\"coerce\")\n",
    "bus[BUS_LON] = pd.to_numeric(bus[BUS_LON], errors=\"coerce\")\n",
    "bus[BUS_PAX] = pd.to_numeric(bus[BUS_PAX], errors=\"coerce\")\n",
    "\n",
    "bus_by_log = bus.set_index(BUS_LOG_ID, drop=False)\n",
    "\n",
    "bus_stop_events = bus.loc[bus[BUS_ARRIVED_STOP].notna(), [BUS_BUS_ID, \"timestamp\", BUS_ARRIVED_STOP, BUS_LAT, BUS_LON]].copy()\n",
    "bus_stop_events = bus_stop_events.rename(columns={BUS_ARRIVED_STOP: \"event_stop_id\", \"timestamp\": \"event_ts\"})\n",
    "\n",
    "bus_sorted = bus.sort_values([BUS_BUS_ID, \"timestamp\"]).reset_index(drop=True)\n",
    "segment_map: Dict[str, Dict[str, np.ndarray]] = {}\n",
    "for bus_id, grp in bus_sorted.groupby(BUS_BUS_ID):\n",
    "    seq = grp.sort_values(\"timestamp\")\n",
    "    ts = seq[\"timestamp\"].values.astype(\"datetime64[ns]\")\n",
    "    lat = seq[BUS_LAT].values if BUS_LAT in seq.columns else None\n",
    "    lon = seq[BUS_LON].values if BUS_LON in seq.columns else None\n",
    "    if len(ts) < 2 or lat is None or lon is None:\n",
    "        segment_map[bus_id] = {\"ts\": np.array([], dtype=\"datetime64[ns]\"),\n",
    "                         \"speed\": np.array([], dtype=float),\n",
    "                         \"dt\": np.array([], dtype=float)}\n",
    "        continue\n",
    "    dt = (ts[1:] - ts[:-1]) / np.timedelta64(1, \"s\")\n",
    "    dist = distance(lat[:-1].astype(float), lon[:-1].astype(float),\n",
    "                                lat[1:].astype(float), lon[1:].astype(float))\n",
    "    speed = dist / dt\n",
    "    segment_map[bus_id] = {\"ts\": ts[1:], \"speed\": speed, \"dt\": dt}\n",
    "\n",
    "def calculate_speed_per_row(bus_id, start_timestamp):\n",
    "    if bus_id not in segment_map:\n",
    "        return np.nan, np.nan\n",
    "    rec = segment_map[bus_id]\n",
    "    seg_ts = rec[\"ts\"]\n",
    "    if seg_ts.size == 0:\n",
    "        return np.nan, np.nan\n",
    "    start = np.datetime64(pd.to_datetime(start_timestamp).to_datetime64())\n",
    "    index = np.searchsorted(seg_ts, start) - 1\n",
    "    speed_prev = np.nan\n",
    "    if index >= 0:\n",
    "        if rec[\"dt\"][index] > 0 and rec[\"dt\"][index] <= 30:\n",
    "            speed_prev = float(rec[\"speed\"][index])\n",
    "    lower = start - np.timedelta64(60, \"s\")\n",
    "    mask = (seg_ts > lower) & (seg_ts <= start)\n",
    "    if mask.any():\n",
    "        speeds = rec[\"speed\"][mask]\n",
    "        speeds = speeds[np.isfinite(speeds)]\n",
    "        speed_1min = float(np.nanmean(speeds)) if speeds.size else np.nan\n",
    "    else:\n",
    "        speed_1min = np.nan\n",
    "    return speed_prev, speed_1min\n",
    "\n",
    "route_next_map = {}\n",
    "try:\n",
    "    with sqlite3.connect(DB_FILE) as _con:\n",
    "        rs = pd.read_sql_query(\"SELECT * FROM Route_Stops\", _con)\n",
    "except Exception:\n",
    "    try:\n",
    "        with sqlite3.connect(DB_FILE) as _con:\n",
    "            rs = pd.read_sql_query(\"SELECT * FROM route_stops\", _con)\n",
    "    except Exception:\n",
    "        rs = pd.DataFrame(columns=[\"route_id\", \"stop_id\", \"position_on_route\"])\n",
    "\n",
    "rs_route_col = next((c for c in rs.columns if c.lower() in (\"route_id_from_stop\",\"route_myid\",\"route_id\",\"route\")), None)\n",
    "rs_stop_col = next((c for c in rs.columns if c.lower() in (\"stop_id\",\"stop_myid\",\"stop\")), None)\n",
    "rs_pos_col  = next((c for c in rs.columns if c.lower() in (\"position_on_route\",\"position\",\"order\")), None)\n",
    "\n",
    "if not rs.empty and rs_route_col and rs_stop_col and rs_pos_col:\n",
    "    rs[rs_route_col] = rs[rs_route_col].astype(str)\n",
    "    rs[rs_stop_col] = rs[rs_stop_col].astype(str)\n",
    "    rs = rs.sort_values([rs_route_col, rs_pos_col]).reset_index(drop=True)\n",
    "    for rid, g in rs.groupby(rs_route_col):\n",
    "        g2 = g.sort_values(rs_pos_col).reset_index(drop=True)\n",
    "        stops = g2[rs_stop_col].astype(str).tolist()\n",
    "        if not stops:\n",
    "            continue\n",
    "        next = {}\n",
    "        L = len(stops)\n",
    "        if L == 1:\n",
    "            next[stops[0]] = stops[0]\n",
    "        else:\n",
    "            for i in range(L):\n",
    "                if i == L - 1:\n",
    "                    next[stops[i]] = stops[1] \n",
    "                else:\n",
    "                   next[stops[i]] = stops[i + 1]  \n",
    "        route_next_map[str(rid)] = {\"stops\": stops, \"next\": next, \"max_pos\": len(stops) - 1}\n",
    "def find_next_stop_id(route_id, stop_id):\n",
    "    if route_id is None or pd.isna(route_id) or stop_id is None or pd.isna(stop_id):\n",
    "        return None\n",
    "    entry = route_next_map.get(str(route_id))\n",
    "    if not entry:\n",
    "        return None\n",
    "    return entry[\"next\"].get(str(stop_id))\n",
    "\n",
    "features_parts = []\n",
    "with sqlite3.connect(DB_FILE) as con:\n",
    "    select_cols = [f\"{ETA_LOG_ID} AS log_id\", f\"{ETA_STOP} AS stop_id\",\n",
    "                   f\"{ETA_SECONDS} AS eta_seconds\", f\"{ETA_SORT} AS sort_order\"]\n",
    "    sql = f\"SELECT {', '.join(select_cols)} FROM {ETA_TBL}\"\n",
    "    total_eta = pd.read_sql_query(f\"SELECT COUNT(*) AS c FROM {ETA_TBL}\", con).iloc[0, 0]\n",
    "    total_chunks = max(1, math.ceil(total_eta / CHUNKSIZE))\n",
    "    chunk_iter = pd.read_sql_query(sql, con, chunksize=CHUNKSIZE)\n",
    "\n",
    "    base_idx = 0\n",
    "    processed = 0\n",
    "    chunk_idx = 0\n",
    "    for chunk in chunk_iter:\n",
    "        chunk_idx += 1\n",
    "        chunk = chunk.reset_index(drop=True)\n",
    "        chunk[\"eta_row_id\"] = np.arange(len(chunk)) + base_idx\n",
    "        base_idx += len(chunk)\n",
    "        print(f\"Chunk {chunk_idx}/{total_chunks} — rows {processed + 1} / {total_eta}\", flush=True)\n",
    "        cols = [BUS_LOG_ID, BUS_BUS_ID, \"timestamp\", BUS_LAT, BUS_LON, BUS_PAX, BUS_ARRIVED_STOP]\n",
    "        if route_col_in_bus and route_col_in_bus in bus_by_log.columns:\n",
    "            cols.append(route_col_in_bus)\n",
    "        bus_by_log_view = bus_by_log.reset_index(drop=True)[cols]\n",
    "\n",
    "        merged = chunk.merge(\n",
    "            bus_by_log_view,\n",
    "            left_on=\"log_id\",\n",
    "            right_on=BUS_LOG_ID,\n",
    "            how=\"left\",\n",
    "            suffixes=(\"_eta\", \"_bus\")\n",
    "        ).rename(columns={BUS_BUS_ID: \"bus_id\", \"timestamp\": \"start_ts\", BUS_PAX: \"bus_pax\"})\n",
    "\n",
    "        if route_col_in_bus and route_col_in_bus in merged.columns:\n",
    "            merged = merged.rename(columns={route_col_in_bus: \"route_id\"})\n",
    "\n",
    "        merged_valid = merged[merged[\"start_ts\"].notna()].copy()\n",
    "        if BUS_ARRIVED_STOP in merged_valid.columns:\n",
    "            merged_valid = merged_valid[ merged_valid[BUS_ARRIVED_STOP].isna() | (merged_valid[BUS_ARRIVED_STOP] != merged_valid[\"stop_id\"]) ].copy()\n",
    "        if merged_valid.empty:\n",
    "            processed += len(chunk)\n",
    "            continue\n",
    "\n",
    "        bus_events_map = {}\n",
    "        for bus_id, g in bus_stop_events.groupby(BUS_BUS_ID):\n",
    "            g2 = g.sort_values(\"event_ts\")[[\"event_ts\", \"event_stop_id\"]].dropna(subset=[\"event_ts\"]).reset_index(drop=True)\n",
    "            if g2.empty:\n",
    "                continue\n",
    "            bus_events_map[bus_id] = {\n",
    "                \"ts\": g2[\"event_ts\"].values.astype(\"datetime64[ns]\"),\n",
    "                \"stop\": g2[\"event_stop_id\"].astype(object).values\n",
    "            }\n",
    "\n",
    "        def lookup_arrival_ts(bid, stop_id, start_ts, route_id=None):\n",
    "            if bid not in bus_events_map or pd.isna(stop_id) or pd.isna(start_ts):\n",
    "                return pd.NaT\n",
    "            arr = bus_events_map[bid]\n",
    "            try:\n",
    "                st64 = np.datetime64(pd.to_datetime(start_ts).to_datetime64())\n",
    "            except Exception:\n",
    "                return pd.NaT\n",
    "            index = np.searchsorted(arr[\"ts\"], st64)\n",
    "            next_stop = find_next_stop_id(route_id, stop_id) if route_id is not None else None\n",
    "            target_stop_str = str(stop_id)\n",
    "            while index < arr[\"ts\"].size:\n",
    "                event_stop = str(int(arr[\"stop\"][index]))\n",
    "                delta_s = (arr[\"ts\"][index] - st64) / np.timedelta64(1, \"s\")\n",
    "                if next_stop is not None and event_stop == str(next_stop):\n",
    "                    return pd.NaT\n",
    "                if event_stop == target_stop_str:\n",
    "                    if 0 < delta_s <= MAX_TIME_BEFORE_STOP:\n",
    "                        return pd.to_datetime(arr[\"ts\"][index])\n",
    "                    return pd.NaT\n",
    "                if delta_s > MAX_TIME_BEFORE_STOP:\n",
    "                    return pd.NaT\n",
    "                index += 1\n",
    "            return pd.NaT\n",
    "        merged_valid[\"actual_arrival_ts\"] = merged_valid.apply(\n",
    "            lambda r: lookup_arrival_ts(r[\"bus_id\"], r[\"stop_id\"], r[\"start_ts\"], route_id=r.get(\"route_id\")),\n",
    "            axis=1\n",
    "        )\n",
    "        merged_valid[\"pred_eta_s\"] = pd.to_numeric(merged_valid[\"eta_seconds\"], errors=\"coerce\")\n",
    "        merged_valid[\"actual_travel_s\"] = (pd.to_datetime(merged_valid[\"actual_arrival_ts\"]) - pd.to_datetime(merged_valid[\"start_ts\"])).dt.total_seconds()\n",
    "        merged_valid[\"eta_error_s\"] = np.where(merged_valid[\"actual_travel_s\"].notna() & merged_valid[\"pred_eta_s\"].notna(),\n",
    "                                               merged_valid[\"actual_travel_s\"] - merged_valid[\"pred_eta_s\"],\n",
    "                                               np.nan)\n",
    "        merged_valid[\"pax_load\"] = pd.to_numeric(merged_valid.get(\"eta_pax\"), errors=\"coerce\")\n",
    "        merged_valid.loc[merged_valid[\"pax_load\"].isna(), \"pax_load\"] = merged_valid.loc[merged_valid[\"pax_load\"].isna(), \"bus_pax\"]\n",
    "        merged_valid[\"start_ts\"] = pd.to_datetime(merged_valid[\"start_ts\"])\n",
    "        merged_valid[\"hour\"] = merged_valid[\"start_ts\"].dt.hour\n",
    "        merged_valid[\"time_of_day_s\"] = merged_valid[\"hour\"] * 3600 + merged_valid[\"start_ts\"].dt.minute * 60 + merged_valid[\"start_ts\"].dt.second\n",
    "\n",
    "        sp_prev_list = []\n",
    "        sp_1min_list = []\n",
    "        for _, row in merged_valid.iterrows():\n",
    "            bus_id = row[\"bus_id\"]\n",
    "            st = row[\"start_ts\"]\n",
    "            sp_prev, sp_1min = calculate_speed_per_row(bus_id, st)\n",
    "            sp_prev_list.append(sp_prev)\n",
    "            sp_1min_list.append(sp_1min)\n",
    "        merged_valid[\"speed_prev_mps\"] = sp_prev_list\n",
    "        merged_valid[\"speed_1min_mps\"] = sp_1min_list\n",
    "\n",
    "        out = merged_valid[[\n",
    "            \"eta_row_id\", \"log_id\", \"bus_id\", \"stop_id\", \"sort_order\", \"eta_seconds\", \"pred_eta_s\",\n",
    "            \"actual_arrival_ts\", \"actual_travel_s\", \"eta_error_s\",\n",
    "            \"pax_load\", \"hour\", \"time_of_day_s\", \"speed_prev_mps\", \"speed_1min_mps\"\n",
    "        ]].copy()\n",
    "        features_parts.append(out)\n",
    "        processed += len(chunk)\n",
    "        print(f\"[ETA] completed {processed} / {total_eta} rows ({chunk_idx}/{total_chunks} chunks)\", flush=True)\n",
    "if features_parts:\n",
    "    features = pd.concat(features_parts, ignore_index=True)\n",
    "else:\n",
    "    features = pd.DataFrame(columns=[\"eta_row_id\",\"log_id\",\"bus_id\",\"stop_id\",\"sort_order\",\"eta_seconds\",\"pred_eta_s\",\n",
    "                                     \"actual_arrival_ts\",\"actual_travel_s\",\"eta_error_s\",\"pax_load\",\"hour\",\n",
    "                                     \"time_of_day_s\",\"speed_prev_mps\",\"speed_1min_mps\"])\n",
    "\n",
    "features = features.sort_values([\"bus_id\",\"eta_row_id\"]).reset_index(drop=True)\n",
    "print(\"Prepared features rows:\", len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1f5c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: 11017166 rows -> features_clean: 7178015 rows\n",
      "   eta_row_id  log_id  bus_id  stop_id  sort_order  eta_seconds  pred_eta_s  \\\n",
      "0      293333   23284    4850    10039           0           92          92   \n",
      "1      293335   23284    4850    10060           2          497         497   \n",
      "2      293336   23284    4850    10035           3          812         812   \n",
      "3      293337   23284    4850    27767           4         1000        1000   \n",
      "4      293338   23284    4850    10038           5         1159        1159   \n",
      "\n",
      "    actual_arrival_ts  actual_travel_s  eta_error_s  pax_load  hour  \\\n",
      "0 2025-11-10 12:21:23            167.0         75.0       4.0    12   \n",
      "1 2025-11-10 12:25:14            398.0        -99.0       4.0    12   \n",
      "2 2025-11-10 12:30:45            729.0        -83.0       4.0    12   \n",
      "3 2025-11-10 12:35:59           1043.0         43.0       4.0    12   \n",
      "4 2025-11-10 12:38:20           1184.0         25.0       4.0    12   \n",
      "\n",
      "   time_of_day_s  speed_prev_mps  speed_1min_mps  \n",
      "0          44316        0.053701        0.599967  \n",
      "1          44316        0.053701        0.599967  \n",
      "2          44316        0.053701        0.599967  \n",
      "3          44316        0.053701        0.599967  \n",
      "4          44316        0.053701        0.599967  \n"
     ]
    }
   ],
   "source": [
    "#DRops rows where any data is missing\n",
    "features_clean = features.dropna(how=\"any\").reset_index(drop=True)\n",
    "print(f\"features: {len(features)} rows -> features_clean: {len(features_clean)} rows\")\n",
    "print(features_clean.head())\n",
    "features_clean.to_csv(\"features_clean_updated_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448b9e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(DB_FILE) as con:\n",
    "    routes = pd.read_sql_query(\"SELECT route_myid, short_name FROM Routes\", con)\n",
    "    lx_route_myids = routes[routes[\"short_name\"].astype(str).str.upper() == \"LX\"][\"route_myid\"].unique()\n",
    "    bus_route_map = pd.read_sql_query(\"SELECT log_id, route_myid FROM Bus_Logs\", con)\n",
    "lx_log_ids = bus_route_map[bus_route_map[\"route_myid\"].isin(lx_route_myids)][\"log_id\"].unique()\n",
    "lx_features = features_clean[features_clean[\"log_id\"].isin(lx_log_ids)].copy()\n",
    "lx_features.to_csv(\"lx_features_updated_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ee3a89",
   "metadata": {},
   "source": [
    "We clean the LX dataset so that it drops every row that contains at least one NA feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022663c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(DB_FILE) as con:\n",
    "    routes = pd.read_sql_query(\"SELECT route_myid, short_name FROM Routes\", con)\n",
    "    b_route_myids = routes[routes[\"short_name\"].astype(str).str.upper() == \"B\"][\"route_myid\"].unique()\n",
    "    bus_route_map = pd.read_sql_query(\"SELECT log_id, route_myid FROM Bus_Logs\", con)\n",
    "b_log_ids = bus_route_map[bus_route_map[\"route_myid\"].isin(b_route_myids)][\"log_id\"].unique()\n",
    "b_features = features_clean[features_clean[\"log_id\"].isin(b_log_ids)].copy()\n",
    "b_features.to_csv(\"b_features_2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
